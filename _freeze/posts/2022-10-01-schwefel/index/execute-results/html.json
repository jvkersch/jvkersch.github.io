{
  "hash": "36135aa13c0c4fcf3c3923c27766c8dc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: The Schwefel function\nimage: schwefel.png\nformat:\n  html:\n    code-fold: true\nbibliography: schwefel.bib\n---\n\n## Introduction\n\nMany optimization algorithms, especially in differential evolution and swarm intelligence, test their performance on a bunch of standard objective functions. Most of these functions are either straightforward (such as the spherical function, which is just the distance to the origin) or very well known (such as the Rastrigin function, which has [its own Wikipedia page](https://en.wikipedia.org/wiki/Rastrigin_function)). \n\nOne function that stands out as being both somewhat mysterious and not easy to reason about right away is the Schwefel function. The form that is usually described [in the literature](https://www.sfu.ca/~ssurjano/schwef.html) has these weird constants in it, and the location of the global minimum likewise is also given numerically as $x_1 = \\cdots = x_n = 420.9687$), to limited precision. This made me curious: what's up with the Schwefel function? Where did it come from? Can we be more precise about the location of its minima? \n\n## Mathematical description\n\nThe Schwefel function is given by\n$$\n    F(x) = - \\sum_{i = 1}^N x_i \\sin \\sqrt{|x_i|}.\n$$\nSome references add $418.9829N$ to this expression, so that the global minimum has function value roughly equal to zero. I won't do that, but I'll just accept that the function changes with increasing $N$.\n\nUsually, the Schwefel function is enclosed in a box centered on the origin, of 1000 units on each side, i.e. $-500 \\le x_i \\le 500$ for all $i = 1, \\ldots, N$. The function has many local minima, as well as one global minimum at $x_1 = \\cdots = x_n = 420.9687$, near the boundary of the domain. This makes it an interesting function to test optimization algorithms on, as it is easy for an algorithm to (a) fail to explore regions near the boundary of the domain, or (b) get stuck in another local minimum.\n\nThe Schwefel appears first in a book by (who else) Schwefel from 1977 [see\n@1977-schwefel-NumerischeOptimierungComputerModellen, problem 2.3 in A1.2] but this book is not often cited. In fact, Google Scholar gives me no citations until 1991, when the evolutionary computing community picked up on it [@1991-muhlenbein-ParallelGeneticAlgorithm is the first reference I could find]. Nowadays, it is used without citation, which is somewhat regrettable since it is not a widely known function.\n\n::: {#bbad1e3f .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\n\ndef schwefel(x, y):\n    return -x*np.sin(np.sqrt(np.abs(x))) - y*np.sin(np.sqrt(np.abs(y)))\n\nr = 500\nx = np.linspace(-r, r, 100)\ny = np.linspace(-r, r, 100)\nX, Y = np.meshgrid(x, y)\nZ = schwefel(X, Y)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The Schwefel function in 2D.](index_files/figure-html/cell-2-output-1.png){fig-align='center'}\n:::\n:::\n\n\nI was interested in finding the exact coordinates of the global minimum, as well as the value of the objective function at that point. To get started, observe that $F(x)$ is the sum of a bunch of univariate functions:\n$$\n    F(x) = f(x_1) + \\cdots + f(x_n),\n$$\nwhere $f(x) = -x \\sin\\sqrt{|x|}$. Consequently, the gradient of $F$ is given by\n$$\n    \\nabla F = \\left[\n        \\begin{matrix}\n            f'(x_1) \\\\\n            \\vdots \\\\\n            f'(x_n)\n        \\end{matrix}\n    \\right],\n$$\nwhere $f'(x)$ is the derivative of $f(x)$. If we want to find the points where $\\nabla F$ vanishes, we therefore have to find the zeros of $f'(x)$, and solving a one-dimensional equation (even if it is nonlinear) is of course much easier than solving a system of nonlinear equations.\n\n## Finding the minima of $f(x)$\n\nTo find the zeros of $f'(x)$, we may assume that $x > 0$ (the case $x < 0$ is similar), so that\n$$\n    f'(x) = - \\sin\\sqrt{x} - \\frac{\\sqrt{x}}{2} \\cos \\sqrt{x} = 0.\n$$    \nThis equation can be rewritten as \n$$\n    \\tan \\sqrt{x} + \\frac{\\sqrt{x}}{2} = 0,\n$$\nor, by substituting $y = \\sqrt{x}$, as\n$$\n    -2\\tan y = y.\n$$\nIn other words, we are looking for the fixed points of the function $g(y) = - 2\\tan y$. There are a few things to keep in mind, though.\n\nBy looking at the graph of $-2\\tan y$, we see that there are many fixed points, and in particular, there is one inside each period of the tangent function. We can therefore parametrize these fixed points as $y = z + k \\pi$, where $k$ is an integer and $z \\in (-\\pi/2, \\pi/2)$. It turns out it will be easier to fix $k$, and look for $z$ inside one fundamental period of the tangent, by solving\n$$\n    \\tan z = - \\frac{z + k \\pi}{2}.\n$$\n\nThis equation has a unique fixed point, but it is unstable (since $|g'(y)| > 1$). To work around this, we take the arctan of both sides to get\n$$\n    z = - \\arctan\\left( \\frac{z + k \\pi}{2} \\right).\n$$\nThis gives us a fixed-point equation with a unique fixed point that is stable (attracting), so we can solve this e.g. by fixed-point iteration. Once we have a solution $z = z_\\text{ext}$, the corresponding $x$ can then be done by putting\n$$\n    x_\\text{ext} = (z_\\text{ext} + k \\pi)^2.\n$$\nThe fixed-point equation has a few interesting properties: \n\n1. For $k > 0$, the solution $z_\\text{ext}$ will be negative: $z_\\text{ext} \\in (-\\pi/2, 0)$. \n2. As $k$ increases, $z_\\text{ext}$ will tend towards $-\\pi/2$. \n\nBoth of these properties follow from the graph of the arctan function shifted over $k \\pi$ units to the left.\n\nFurthermore, by substituting the expression for the solution back into $f(x)$ and using these sign properties, we get that\n$$\n    f(x_\\text{ext}) = (-1)^k x_\\text{ext}\\sqrt{\\frac{x_\\text{ext}}{4 + x_\\text{ext}}}.\n$$\nIn other words, we get an alternating series of minima (for $k$ odd) and maxima (for $k$ even), whose magnitude increases with increasing $k$.\n\nThe table below lists the first few zeros of $f'(x)$, together with the value of $f(x)$.\n\n::: {#tbl-zeros .cell tbl-cap='Extrema and function values for the 1D Schwefel function.' execution_count=2}\n``` {.python .cell-code}\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\nfrom math import atan, pi\nfrom scipy.optimize import fixed_point\n\ndef schwefel_1d(x):\n    return -x*np.sin(np.abs(x)**0.5)\n\ndef solve_fixed_point(k):\n    def fpe(z):\n        return -atan((z + k*pi)/2)\n    z_optim = fixed_point(fpe, 0)[()]\n    x_optim = (z_optim + k*pi)**2\n    return x_optim, schwefel_1d(x_optim)\n\ntable = [(k,) + solve_fixed_point(k) for k in range(1, 8)]\nMarkdown(tabulate(\n    table, \n    headers=[\"Index\", \"x\", \"f(x)\"],\n    floatfmt=\".8f\"\n))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=2}\n  Index             x           f(x)\n-------  ------------  -------------\n      1    5.23919930    -3.94530163\n      2   25.87741735    24.08296022\n      3   65.54786509   -63.63498195\n      4  124.82935642   122.87617351\n      5  203.81425265  -201.84321788\n      6  302.52493561   300.54455266\n      7  420.96874636  -418.98288727\n:::\n:::\n\n\n::: {#fbc3004a .cell execution_count=3}\n``` {.python .cell-code}\nxs = np.linspace(0, 500, 500)\nys = schwefel_1d(xs)\n\nplt.plot(xs, ys)\nplt.plot([item[1] for item in table],\n         [item[2] for item in table],\n         \"ro\")\n```\n\n::: {.cell-output .cell-output-display}\n![Extrema of the 1D Schwefel function.](index_files/figure-html/cell-4-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## Extrema of the Schwefel function\n\nThis tells us everything we need to know about minima and maxima of $F$. First of all, we can find all (coordinate-wise positive) extrema of $F(x)$ by finding all of the zeros of the aforementioned fixed-point equation (see @tbl-zeros), and then choosing (with replacement) $n$ of these zeros and assembling them into a coordinate vector. Each such $n$-vector is a zero of $\\nabla F$. In 2D, this gives the distribution of extrema as shown below.\n\n::: {#6307cda6 .cell execution_count=4}\n``` {.python .cell-code}\nr = 500\nx = np.linspace(-r, r, 100)\ny = np.linspace(-r, r, 100)\nX, Y = np.meshgrid(x, y)\nZ = schwefel(X, Y)\n    \nplt.figure(figsize=(7, 7))\nplt.contour(X, Y, Z, levels=20, cmap=cm.coolwarm)\n\nextrema = np.asarray([item[1] for item in table])\nXext, Yext = np.meshgrid(extrema, extrema)\n\ndef s(x, y):\n    plt.scatter(x, y, fc='gray', ec='black', zorder=2)\n\ns(Xext, Yext)\ns(Xext, -Yext)\ns(-Xext, Yext)\ns(-Xext, -Yext)\n```\n\n::: {.cell-output .cell-output-display}\n![Extrema of the 1D Schwefel function.](index_files/figure-html/cell-5-output-1.png){fig-align='center'}\n:::\n:::\n\n\nThe remaining question is which of these extrema provides the *global* minimum. First of all, note that $F$ being the sum of $n$ copies of $f$ tells us that the global minimum must have $x_1 = \\cdots = x_n = x_\\text{min}$, with $x_\\text{min}$ the global minimum of $f(x)$. So we can reduce our $n$-dimensional minimization problem to a one-dimensional one, for which we have the fixed point equation.\n\nSecondly, where is $x_\\text{min}$ located? For this, we use the expression for the minima and maxima derived earlier. We need to find the largest odd value for $k$ such that $x_\\text{ext}$ is still within our domain. Since our domain is limited by $x = 500$, this gives us $k = 7$.\n\nNow, we either solve the fixed-point equation for $k = 7$, or we consult table @tbl-zeros. Either way, we get\n$$\n    x_\\text{ext} = (z_\\text{ext} + 7 \\pi)^2 \\approx 420.96874635998194.\n$$\nThis is precisely the value quoted in various sources, to greater accuracy. This approximation is correct to about 8 decimal places (at least), corresponding to the default accuracy of SciPy's [`fixed_point`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fixed_point.html) solver. A higher-accuracy approximation is given in the next section.\n\n## High-precision location of the extremum\n\n*Added 2024-01-03.*\n\nUsing the [mpmath](https://mpmath.org) library for arbitrary-precision floating point arithmetic, we can find the value for $x_\\text{ext}$ to very high precision, in this case to approximately 50 decimal places. This value can be useful e.g. when calibrating your optimizer.\n\n::: {#aa75b5cb .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nfrom mpmath import mp\nmp.dps = 60\n\nz = mp.mpf(0)\nfor _ in range(50):\n    # each iteration gives us 2 decimal places, so a fixed \n    # number of 50 iterations should be more than enough.\n    z = -mp.atan((z + 7*mp.pi)/2)\n\nx = (z + 7*mp.pi)**2\nprint(f\"x = {x}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx = 420.968746359982027311844365018686486001674755877017274182781\n```\n:::\n:::\n\n\nWith this value for $x_\\text{ext}$, $f'(x)$ becomes vanishingly small, indicating that we're indeed right at the extremum.\n\n::: {#a960c70c .cell execution_count=6}\n``` {.python .cell-code}\nimport mpmath\n\nresid = -mp.sin(x**0.5) - x**0.5/2 * mp.cos(x**0.5)\nprint(f\"f'(x) = {mpmath.nstr(resid)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nf'(x) = -7.00089e-61\n```\n:::\n:::\n\n\n## Approximate locations of the extrema\n\n[@1977-schwefel-NumerischeOptimierungComputerModellen] has the following approximate expression for the extrema:\n$$\n\tx_\\text{ext} \\approx \\pm \\pi^2\\left(\\frac{1}{2} + k\\right)^2,\n$$\nvalid when $k$ is large. This follows easily from the fixed-point equation: for $k$ large, $\\arctan\\left( \\frac{z + k \\pi}{2} \\right)$ tends to $-\\pi/2$, so that $z_\\text{ext} \\approx \\pi/2$, and the approximation for $x_\\text{ext}$ follows.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}