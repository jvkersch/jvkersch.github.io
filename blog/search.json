[
  {
    "objectID": "posts/2022-10-01-schwefel/index.html",
    "href": "posts/2022-10-01-schwefel/index.html",
    "title": "The Schwefel function",
    "section": "",
    "text": "Many optimization algorithms, especially in differential evolution and swarm intelligence, test their performance on a bunch of standard objective functions. Most of these functions are either straightforward (such as the spherical function, which is just the distance to the origin) or very well known (such as the Rastrigin function, which has its own Wikipedia page).\nOne function that stands out as being both somewhat mysterious and not easy to reason about right away is the Schwefel function. The form that is usually described in the literature has these weird constants in it, and the location of the global minimum likewise is also given numerically as \\(x_1 = \\cdots = x_n = 420.9687\\)), to limited precision. This made me curious: what’s up with the Schwefel function? Where did it come from? Can we be more precise about the location of its minima?"
  },
  {
    "objectID": "posts/2022-10-01-schwefel/index.html#mathematical-description",
    "href": "posts/2022-10-01-schwefel/index.html#mathematical-description",
    "title": "The Schwefel function",
    "section": "Mathematical description",
    "text": "Mathematical description\nThe Schwefel function is given by\n\\[\n    F(x) = - \\sum_{i = 1}^N x_i \\sin \\sqrt{|x_i|}.\n\\]\nSome references add \\(418.9829N\\) to this expression, so that the global minimum has function value roughly equal to zero. I won’t do that, but I’ll just accept that the function changes with increasing \\(N\\).\nUsually, the Schwefel function is enclosed in a box centered on the origin, of 1000 units on each side, i.e. \\(-500 \\le x_i \\le 500\\) for all \\(i = 1, \\ldots, N\\). The function has many local minima, as well as one global minimum at \\(x_1 = \\cdots = x_n = 420.9687\\), near the boundary of the domain. This makes it an interesting function to test optimization algorithms on, as it is easy for an algorithm to (a) fail to explore regions near the boundary of the domain, or (b) get stuck in another local minimum.\nThe Schwefel appears first in a book by (who else) Schwefel from 1977 (see Schwefel 1977, problem 2.3 in A1.2) but this book is not often cited. In fact, Google Scholar gives me no citations until 1991, when the evolutionary computing community picked up on it (Mühlenbein, Schomisch, and Born 1991 is the first reference I could find). Nowadays, it is used without citation, which is somewhat regrettable since it is not a widely known function.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\n\ndef schwefel(x, y):\n    return -x*np.sin(np.sqrt(np.abs(x))) - y*np.sin(np.sqrt(np.abs(y)))\n\nr = 500\nx = np.linspace(-r, r, 100)\ny = np.linspace(-r, r, 100)\nX, Y = np.meshgrid(x, y)\nZ = schwefel(X, Y)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)\n\nplt.show()\n\n\n\n\n\nThe Schwefel function in 2D.\n\n\n\n\nI was interested in finding the exact coordinates of the global minimum, as well as the value of the objective function at that point. To get started, observe that \\(F(x)\\) is the sum of a bunch of univariate functions:\n\\[\n    F(x) = f(x_1) + \\cdots + f(x_n),\n\\]\nwhere \\(f(x) = -x \\sin\\sqrt{|x|}\\). Consequently, the gradient of \\(F\\) is given by\n\\[\n    \\nabla F = \\left[\n        \\begin{matrix}\n            f'(x_1) \\\\\n            \\vdots \\\\\n            f'(x_n)\n        \\end{matrix}\n    \\right],\n\\]\nwhere \\(f'(x)\\) is the derivative of \\(f(x)\\). If we want to find the points where \\(\\nabla F\\) vanishes, we therefore have to find the zeros of \\(f'(x)\\), and solving a one-dimensional equation (even if it is nonlinear) is of course much easier than solving a system of nonlinear equations."
  },
  {
    "objectID": "posts/2022-10-01-schwefel/index.html#finding-the-minima-of-fx",
    "href": "posts/2022-10-01-schwefel/index.html#finding-the-minima-of-fx",
    "title": "The Schwefel function",
    "section": "Finding the minima of \\(f(x)\\)",
    "text": "Finding the minima of \\(f(x)\\)\nTo find the zeros of \\(f'(x)\\), we may assume that \\(x > 0\\) (the case \\(x < 0\\) is similar), so that\n\\[\n    f'(x) = - \\sin\\sqrt{x} - \\frac{\\sqrt{x}}{2} \\cos \\sqrt{x} = 0.\n\\]\nThis equation can be rewritten as\n\\[\n    \\tan \\sqrt{x} + \\frac{\\sqrt{x}}{2} = 0,\n\\]\nor, by substituting \\(y = \\sqrt{x}\\), as\n\\[\n    -2\\tan y = y.\n\\]\nIn other words, we are looking for the fixed points of the function \\(g(y) = - 2\\tan y\\). There are a few things to keep in mind, though.\nBy looking at the graph of \\(-2\\tan y\\), we see that there are many fixed points, and in particular, there is one inside each period of the tangent function. We can therefore parametrize these fixed points as \\(y = z + k \\pi\\), where \\(k\\) is an integer and \\(z \\in (-\\pi/2, \\pi/2)\\). It turns out it will be easier to fix \\(k\\), and look for \\(z\\) inside one fundamental period of the tangent, by solving\n\\[\n    \\tan z = - \\frac{z + k \\pi}{2}.\n\\]\nThis equation has a unique fixed point, but it is unstable (since \\(|g'(y)| > 1\\)). To work around this, we take the arctan of both sides to get\n\\[\n    z = - \\arctan\\left( \\frac{z + k \\pi}{2} \\right).\n\\]\nThis gives us a fixed-point equation with a unique fixed point that is stable (attracting), so we can solve this e.g. by fixed-point iteration. Once we have a solution \\(z = z_\\text{ext}\\), the corresponding \\(x\\) can then be done by putting\n\\[\n    x_\\text{ext} = (z_\\text{ext} + k \\pi)^2.\n\\]\nThe fixed-point equation has a few interesting properties:\n\nFor \\(k > 0\\), the solution \\(z_\\text{ext}\\) will be negative: \\(z_\\text{ext} \\in (-\\pi/2, 0)\\).\nAs \\(k\\) increases, \\(z_\\text{ext}\\) will tend towards \\(-\\pi/2\\).\n\nBoth of these properties follow from the graph of the arctan function shifted over \\(k \\pi\\) units to the left.\nFurthermore, by substituting the expression for the solution back into \\(f(x)\\) and using these sign properties, we get that\n\\[\n    f(x_\\text{ext}) = (-1)^k x_\\text{ext}\\sqrt{\\frac{x_\\text{ext}}{4 + x_\\text{ext}}}.\n\\]\nIn other words, we get an alternating series of minima (for \\(k\\) odd) and maxima (for \\(k\\) even), whose magnitude increases with increasing \\(k\\).\nThe table below lists the first few zeros of \\(f'(x)\\), together with the value of \\(f(x)\\).\n\n\nCode\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\nfrom math import atan, pi\nfrom scipy.optimize import fixed_point\n\ndef schwefel_1d(x):\n    return -x*np.sin(np.abs(x)**0.5)\n\ndef solve_fixed_point(k):\n    def fpe(z):\n        return -atan((z + k*pi)/2)\n    z_optim = fixed_point(fpe, 0)[()]\n    x_optim = (z_optim + k*pi)**2\n    return x_optim, schwefel_1d(x_optim)\n\ntable = [(k,) + solve_fixed_point(k) for k in range(1, 8)]\nMarkdown(tabulate(\n    table, \n    headers=[\"Index\", \"x\", \"f(x)\"],\n    floatfmt=\".8f\"\n))\n\n\n\n\nTable 1: Extrema and function values for the 1D Schwefel function.\n\n\nIndex\nx\nf(x)\n\n\n\n\n1\n5.23919930\n-3.94530163\n\n\n2\n25.87741735\n24.08296022\n\n\n3\n65.54786509\n-63.63498195\n\n\n4\n124.82935642\n122.87617351\n\n\n5\n203.81425265\n-201.84321788\n\n\n6\n302.52493561\n300.54455266\n\n\n7\n420.96874636\n-418.98288727\n\n\n\n\n\n\n\n\nCode\nxs = np.linspace(0, 500, 500)\nys = schwefel_1d(xs)\n\nplt.plot(xs, ys)\nplt.plot([item[1] for item in table],\n         [item[2] for item in table],\n         \"ro\")\n\n\n\n\n\nExtrema of the 1D Schwefel function."
  },
  {
    "objectID": "posts/2022-10-01-schwefel/index.html#extrema-of-the-schwefel-function",
    "href": "posts/2022-10-01-schwefel/index.html#extrema-of-the-schwefel-function",
    "title": "The Schwefel function",
    "section": "Extrema of the Schwefel function",
    "text": "Extrema of the Schwefel function\nThis tells us everything we need to know about minima and maxima of \\(F\\). First of all, we can find all (coordinate-wise positive) extrema of \\(F(x)\\) by finding all of the zeros of the aforementioned fixed-point equation (see Table 1), and then choosing (with replacement) \\(n\\) of these zeros and assembling them into a coordinate vector. Each such \\(n\\)-vector is a zero of \\(\\nabla F\\). In 2D, this gives the distribution of extrema as shown below.\n\n\nCode\nr = 500\nx = np.linspace(-r, r, 100)\ny = np.linspace(-r, r, 100)\nX, Y = np.meshgrid(x, y)\nZ = schwefel(X, Y)\n    \nplt.figure(figsize=(7, 7))\nplt.contour(X, Y, Z, levels=20, cmap=cm.coolwarm)\n\nextrema = np.asarray([item[1] for item in table])\nXext, Yext = np.meshgrid(extrema, extrema)\n\ndef s(x, y):\n    plt.scatter(x, y, fc='gray', ec='black', zorder=2)\n\ns(Xext, Yext)\ns(Xext, -Yext)\ns(-Xext, Yext)\ns(-Xext, -Yext)\n\n\n\n\n\nExtrema of the 1D Schwefel function.\n\n\n\n\nThe remaining question is which of these extrema provides the global minimum. First of all, note that \\(F\\) being the sum of \\(n\\) copies of \\(f\\) tells us that the global minimum must have \\(x_1 = \\cdots = x_n = x_\\text{min}\\), with \\(x_\\text{min}\\) the global minimum of \\(f(x)\\). So we can reduce our \\(n\\)-dimensional minimization problem to a one-dimensional one, for which we have the fixed point equation.\nSecondly, where is \\(x_\\text{min}\\) located? For this, we use the expression for the minima and maxima derived earlier. We need to find the largest odd value for \\(k\\) such that \\(x_\\text{ext}\\) is still within our domain. Since our domain is limited by \\(x = 500\\), this gives us \\(k = 7\\).\nNow, we either solve the fixed-point equation for \\(k = 7\\), or we consult table Table 1. Either way, we get\n\\[\n    x_\\text{ext} = (z_\\text{ext} + 7 \\pi)^2 \\approx 420.96874635998194.\n\\]\nThis is precisely the value quoted in various sources, to greater accuracy."
  },
  {
    "objectID": "posts/2022-10-01-schwefel/index.html#approximate-locations-of-the-extrema",
    "href": "posts/2022-10-01-schwefel/index.html#approximate-locations-of-the-extrema",
    "title": "The Schwefel function",
    "section": "Approximate locations of the extrema",
    "text": "Approximate locations of the extrema\n(Schwefel 1977) has the following approximate expression for the extrema:\n\\[\n    x_\\text{ext} \\approx \\pm \\pi^2\\left(\\frac{1}{2} + k\\right)^2,\n\\]\nvalid when \\(k\\) is large. This follows easily from the fixed-point equation: for \\(k\\) large, \\(\\arctan\\left( \\frac{z + k \\pi}{2} \\right)\\) tends to \\(-\\pi/2\\), so that \\(z_\\text{ext} \\approx \\pi/2\\), and the approximation for \\(x_\\text{ext}\\) follows.\nUnfortunately, this approximation is not very accurate for the range that we’re interested. For example, for \\(k = 7\\) we get that \\(x_\\text{ext} \\approx (7.5 \\pi)^2 = 555.165\\), which is nowhere near the true value. We need to take \\(k = 201\\) before we get something that is 1% accurate.\nA better approximation can be obtained by expanding \\(\\arctan x\\) around \\(x = \\infty\\) to first order as \\(\\arctan x = \\pi/2 + 1/x + O(x^{-3})\\). Using this approximation in the fixed-point equation and solving the resulting quadratic equation gives\n\\[\n    x_\\text{ext} \\approx \\pm \\frac{1}{16}\\left(\n        (2k-1)\\pi + \\sqrt{32 + (2k-1)^2 \\pi^2} \\right)^2.\n\\]\nFor \\(k = 7\\), this gives \\(x_\\text{ext} \\approx 420.9813\\), which is indeed quite close."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]